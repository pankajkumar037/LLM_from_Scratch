{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sqqCZoB5upn",
        "outputId": "df1e35f2-9fe7-4a2e-fabf-bdbea3e40204"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "tokenizer=tiktoken.get_encoding(\"o200k_base\")"
      ],
      "metadata": {
        "id": "QulxG0ke5zKY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  raw_text=f.read()"
      ],
      "metadata": {
        "id": "psCT11zYxyQN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_text=tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9occneFx5SfT",
        "outputId": "fb1798c0-4ee9-4b48-dc2c-327623146a27"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executing the code above will return 4836, the total number of tokens or Vocab size  in the training set,\n",
        "after applying the BPE tokenizer ."
      ],
      "metadata": {
        "id": "lqHB3sV_68Kf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "Next, we remove the first 50 tokens from the dataset for demonstration purposes as it\n",
        "results in a slightly more interesting text passage in the next steps:</div>"
      ],
      "metadata": {
        "id": "eV6z322-7PCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample=enc_text[50:]"
      ],
      "metadata": {
        "id": "abpHCa7J673-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#One of the easiest and most intuitive ways to create the input-target pairs for the nextword prediction task is to create two variables, x and y, where x contains the input tokens\n",
        "#and y contains the targets, which are the inputs shifted by 1"
      ],
      "metadata": {
        "id": "gO9LSb49670k"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4 #length of the input\n",
        "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens)\n",
        "#to predict the next word in the sequence.\n",
        "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn5_L_Al6sp8",
        "outputId": "9af4724c-824f-4fe5-881e-b6496a6fa0fd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [11166, 306, 261, 38350]\n",
            "y:      [306, 261, 38350, 402]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#context size means how many words the model should pay attention at time of orediction of next words"
      ],
      "metadata": {
        "id": "kMfQwK_l6Zjd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "Processing the inputs along with the targets, which are the inputs shifted by one position,\n",
        "we can then create the next-word prediction tasks as\n",
        "follows:</div>"
      ],
      "metadata": {
        "id": "uUiEBSdL9lgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,context_size+1):\n",
        "  context=enc_sample[:i]\n",
        "  desired=enc_sample[i]\n",
        "\n",
        "  print(context,\"--->\",desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSxYd01Q5Sj6",
        "outputId": "9baf6d49-ee23-4a0d-9a4d-cb521e4c7d86"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11166] ---> 306\n",
            "[11166, 306] ---> 261\n",
            "[11166, 306, 261] ---> 38350\n",
            "[11166, 306, 261, 38350] ---> 402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#in case of LLM one input o/p pair has context size prediction task\n",
        "#here one ip->op pair has 4 prediction task"
      ],
      "metadata": {
        "id": "AWeg1EvB5SmV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,context_size+1):\n",
        "  context=enc_sample[:i]\n",
        "  desired=enc_sample[i]\n",
        "\n",
        "  print(tokenizer.decode(context),\"--->\",tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpT0M2Xx5Soh",
        "outputId": "0c3245c7-82dc-4285-b31f-33dcc036eba0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " himself --->  in\n",
            " himself in --->  a\n",
            " himself in a --->  villa\n",
            " himself in a villa --->  on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "There's only one more task before we can turn the tokens into embeddings:implementing an efficient data loader that\n",
        "iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which\n",
        "can be thought of as multidimensional arrays.\n",
        "    "
      ],
      "metadata": {
        "id": "Bp-B12exDx4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#In particular, we are interested in returning two tensors: an input tensor containing the\n",
        "#text that the LLM sees and a target tensor that includes the targets for the LLM to predict,"
      ],
      "metadata": {
        "id": "EbQMpHkC5Sqv"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing A Data Loader**"
      ],
      "metadata": {
        "id": "1HJ4GeHfD9Fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Step 1: Tokenize the entire text\n",
        "    \n",
        "Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "\n",
        "Step 3: Return the total number of rows in the dataset\n",
        "\n",
        "Step 4: Return a single row from the dataset\n",
        "</div>"
      ],
      "metadata": {
        "id": "yoML3wKgG09z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "ZzEvu4qfD80t"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#what we have done"
      ],
      "metadata": {
        "id": "Tt6d_xlX5SvR"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "The GPTDatasetV1 class in listing 2.5 is based on the PyTorch Dataset class.\n",
        "\n",
        "It defines how individual rows are fetched from the dataset.\n",
        "\n",
        "Each row consists of a number of\n",
        "token IDs (based on a max_length) assigned to an input_chunk tensor.\n",
        "\n",
        "The target_chunk\n",
        "tensor contains the corresponding targets.\n",
        "\n",
        "I recommend reading on to see how the data\n",
        "returned from this dataset looks like when we combine the dataset with a PyTorch\n",
        "DataLoader -- this will bring additional intuition and clarity.\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "_1cR94rBK4Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The following code will use the GPTDatasetV1 to load the inputs in batches via a PyTorch\n",
        "#DataLoader"
      ],
      "metadata": {
        "id": "F89ZKgg25SxM"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Step 1: Initialize the tokenizer\n",
        "\n",
        "Step 2: Create dataset\n",
        "\n",
        "Step 3: drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes\n",
        "during training\n",
        "\n",
        "Step 4: The number of CPU processes to use for preprocessing\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "5dsQLQWQLE9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initializing the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Creating dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "dCiYuFDX5SzX"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4,\n",
        "\n",
        "#This will develop an intuition of how the GPTDatasetV1 class and the create_dataloader_v1 function work together:"
      ],
      "metadata": {
        "id": "7Shtt-WY5S1e"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ],
      "metadata": {
        "id": "aDygO1T65S4U"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2CVqmwbMSZu",
        "outputId": "b47a601f-dce6-4726-91ad-24b279ac71c8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w90DJmkzMSWZ",
        "outputId": "fe2453a6-38e6-402e-e31e-eca7e8dbfedd"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have previous experience with deep learning, we  know\n",
        "that small batch sizes require less memory during training but lead to more noisy model\n",
        "updates.\n",
        "\n",
        "Just like in regular deep learning, the batch size is a trade-off and hyperparameter\n",
        "to experiment with when training LLMs."
      ],
      "metadata": {
        "id": "awFODZdKNkhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVT0bjHfMSS4",
        "outputId": "f3e1938d-d2ab-40c8-d543-87c4d8446cac"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "shi_UDT1MSPh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}