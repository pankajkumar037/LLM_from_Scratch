{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sqqCZoB5upn",
        "outputId": "c571b4c2-37a7-4017-cc60-2a871408469e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "tokenizer=tiktoken.get_encoding(\"o200k_base\")"
      ],
      "metadata": {
        "id": "QulxG0ke5zKY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  raw_text=f.read()"
      ],
      "metadata": {
        "id": "psCT11zYxyQN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_text=tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9occneFx5SfT",
        "outputId": "042173e5-8913-4a65-c5cb-5c3ce25a68f2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executing the code above will return 4836, the total number of tokens or Vocab size  in the training set,\n",
        "after applying the BPE tokenizer ."
      ],
      "metadata": {
        "id": "lqHB3sV_68Kf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "Next, we remove the first 50 tokens from the dataset for demonstration purposes as it\n",
        "results in a slightly more interesting text passage in the next steps:</div>"
      ],
      "metadata": {
        "id": "eV6z322-7PCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample=enc_text[50:]"
      ],
      "metadata": {
        "id": "abpHCa7J673-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#One of the easiest and most intuitive ways to create the input-target pairs for the nextword prediction task is to create two variables, x and y, where x contains the input tokens\n",
        "#and y contains the targets, which are the inputs shifted by 1"
      ],
      "metadata": {
        "id": "gO9LSb49670k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4 #length of the input\n",
        "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens)\n",
        "#to predict the next word in the sequence.\n",
        "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn5_L_Al6sp8",
        "outputId": "4dd4ab17-6d5c-4ec8-f66e-3d0aea07aa7e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [11166, 306, 261, 38350]\n",
            "y:      [306, 261, 38350, 402]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#context size means how many words the model should pay attention at time of orediction of next words"
      ],
      "metadata": {
        "id": "kMfQwK_l6Zjd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "Processing the inputs along with the targets, which are the inputs shifted by one position,\n",
        "we can then create the next-word prediction tasks as\n",
        "follows:</div>"
      ],
      "metadata": {
        "id": "uUiEBSdL9lgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,context_size+1):\n",
        "  context=enc_sample[:i]\n",
        "  desired=enc_sample[i]\n",
        "\n",
        "  print(context,\"--->\",desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSxYd01Q5Sj6",
        "outputId": "87bd8630-5813-42b7-acc0-3df8a328cfef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11166] ---> 306\n",
            "[11166, 306] ---> 261\n",
            "[11166, 306, 261] ---> 38350\n",
            "[11166, 306, 261, 38350] ---> 402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#in case of LLM one input o/p pair has context size prediction task\n",
        "#here one ip->op pair has 4 prediction task"
      ],
      "metadata": {
        "id": "AWeg1EvB5SmV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,context_size+1):\n",
        "  context=enc_sample[:i]\n",
        "  desired=enc_sample[i]\n",
        "\n",
        "  print(tokenizer.decode(context),\"--->\",tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpT0M2Xx5Soh",
        "outputId": "107119c1-70b3-4601-8ce4-4222c6178b07"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " himself --->  in\n",
            " himself in --->  a\n",
            " himself in a --->  villa\n",
            " himself in a villa --->  on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "There's only one more task before we can turn the tokens into embeddings:implementing an efficient data loader that\n",
        "iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which\n",
        "can be thought of as multidimensional arrays.\n",
        "    "
      ],
      "metadata": {
        "id": "Bp-B12exDx4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#In particular, we are interested in returning two tensors: an input tensor containing the\n",
        "#text that the LLM sees and a target tensor that includes the targets for the LLM to predict,"
      ],
      "metadata": {
        "id": "EbQMpHkC5Sqv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing A Data Loader**"
      ],
      "metadata": {
        "id": "1HJ4GeHfD9Fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Step 1: Tokenize the entire text\n",
        "    \n",
        "Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "\n",
        "Step 3: Return the total number of rows in the dataset\n",
        "\n",
        "Step 4: Return a single row from the dataset\n",
        "</div>"
      ],
      "metadata": {
        "id": "yoML3wKgG09z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "ZzEvu4qfD80t"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#what we have done"
      ],
      "metadata": {
        "id": "Tt6d_xlX5SvR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "The GPTDatasetV1 class in listing 2.5 is based on the PyTorch Dataset class.\n",
        "\n",
        "It defines how individual rows are fetched from the dataset.\n",
        "\n",
        "Each row consists of a number of\n",
        "token IDs (based on a max_length) assigned to an input_chunk tensor.\n",
        "\n",
        "The target_chunk\n",
        "tensor contains the corresponding targets.\n",
        "\n",
        "I recommend reading on to see how the data\n",
        "returned from this dataset looks like when we combine the dataset with a PyTorch\n",
        "DataLoader -- this will bring additional intuition and clarity.\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "_1cR94rBK4Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The following code will use the GPTDatasetV1 to load the inputs in batches via a PyTorch\n",
        "#DataLoader"
      ],
      "metadata": {
        "id": "F89ZKgg25SxM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Step 1: Initialize the tokenizer\n",
        "\n",
        "Step 2: Create dataset\n",
        "\n",
        "Step 3: drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes\n",
        "during training\n",
        "\n",
        "Step 4: The number of CPU processes to use for preprocessing\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "5dsQLQWQLE9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initializing the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Creating dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "dCiYuFDX5SzX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4,\n",
        "\n",
        "#This will develop an intuition of how the GPTDatasetV1 class and the create_dataloader_v1 function work together:"
      ],
      "metadata": {
        "id": "7Shtt-WY5S1e"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ],
      "metadata": {
        "id": "aDygO1T65S4U"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2CVqmwbMSZu",
        "outputId": "bc8037f7-53a5-4084-d3f7-1b7fdbe07256"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w90DJmkzMSWZ",
        "outputId": "28c7ae12-72d6-43a0-99c7-f01c17cc1c45"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have previous experience with deep learning, we  know\n",
        "that small batch sizes require less memory during training but lead to more noisy model\n",
        "updates.\n",
        "\n",
        "Just like in regular deep learning, the batch size is a trade-off and hyperparameter\n",
        "to experiment with when training LLMs."
      ],
      "metadata": {
        "id": "awFODZdKNkhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVT0bjHfMSS4",
        "outputId": "4bce3ff2-9a79-4671-8809-0a0a7ff840e6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CREATING TOKEN EMBEDDINGS"
      ],
      "metadata": {
        "id": "-yFImR1Dl9EG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of\n",
        "only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want\n",
        "to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):"
      ],
      "metadata": {
        "id": "gqJrFIzpmLLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ],
      "metadata": {
        "id": "aey_6SOfl8g6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch,\n",
        "setting the random seed to 123 for reproducibility purposes:\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "0wuevKmSmPzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "3PwKeMQQmPMw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shi_UDT1MSPh",
        "outputId": "360ea0d4-3ac8-4900-8b1b-a9ddc30644b7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([2])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDoHJApUPYaJ",
        "outputId": "40ca6fb2-c1a3-4157-db32-44e6d32c5e10"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76E5CYn_PYXJ",
        "outputId": "8af50c5d-2c46-4b50-bac6-34b5d6f27061"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Each row in this output matrix is obtained via a lookup operation from the embedding #weight matrix"
      ],
      "metadata": {
        "id": "nHzHNEyTPYVX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S0ET7MvNPYT0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **POSITIONAL EMBEDDINGS (ENCODING WORD POSITIONS)**"
      ],
      "metadata": {
        "id": "G8lVNiUiDuw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously, we focused on very small embedding sizes in this chapter for illustration\n",
        "purposes.\n",
        "\n",
        "We now consider more realistic and useful embedding sizes and encode the input\n",
        "tokens into a 256-dimensional vector representation.\n",
        "\n",
        "This is smaller than what the original\n",
        "GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable\n",
        "for experimentation.\n",
        "\n",
        "Furthermore, we assume that the token IDs were created by the BPE\n",
        "tokenizer that we implemented earlier, which has a vocabulary size of 50256:"
      ],
      "metadata": {
        "id": "XWgCe9wFD42A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "FyZsBgFMFPl2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tx=\"hello mu name.<|endoftext|>\"\n",
        "tokenizer.encode(tx,allowed_special={\"<|endoftext|>\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQjqqUwtPYSF",
        "outputId": "5387831f-b087-4316-8c87-e480275479ee"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[31373, 38779, 1438, 13, 50256]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "kckx4nBlPYQb"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the token_embedding_layer above, if we sample data from the data loader, we\n",
        "embed each token in each batch into a 256-dimensional vector. If we have a batch size of 8\n",
        "with four tokens each, the result will be an 8 x 4 x 256 tensor."
      ],
      "metadata": {
        "id": "i5JGvb5qFy6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Let's instantiate the data loader ( Data sampling with a sliding window),\n",
        "first:\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "hSAQUWmTGHOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ],
      "metadata": {
        "id": "yVJ-V8guGGqz"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_JUbBTRFxX8",
        "outputId": "fe6907ad-fc53-4c86-be4b-77bf898f5f65"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now use the embedding layer to embed these token IDs into 256-dimensional\n",
        "vectors:"
      ],
      "metadata": {
        "id": "gh5DFmptGeCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n-0mxN8Gdkx",
        "outputId": "34b8d63b-94dd-4a4e-bffc-f66cd65b46fb"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#each token ID is now embedded as a 256-dimensional vector."
      ],
      "metadata": {
        "id": "dGhJBNbUPYN8"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "For a GPT model's absolute embedding approach, we just need to create another\n",
        "embedding layer that has the same dimension as the token_embedding_layer:\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "lHZtpTBVHVkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ],
      "metadata": {
        "id": "7RgOCT35PYLM"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5ch69IwDsLM",
        "outputId": "dbec19d9-25e4-4168-c9fd-22f9da1bf8ca"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "As shown in the preceding code example, the input to the pos_embeddings is usually a\n",
        "placeholder vector torch.arange(context_length), which contains a sequence of\n",
        "numbers 0, 1, ..., up to the maximum input length âˆ’ 1.\n",
        "\n",
        "The context_length is a variable\n",
        "that represents the supported input size of the LLM.\n",
        "\n",
        "Here, we choose it similar to the\n",
        "maximum length of the input text.\n",
        "\n",
        "In practice, input text can be longer than the supported\n",
        "context length, in which case we have to truncate the text.\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "M6V4G9_0HhhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "As we can see, the positional embedding tensor consists of four 256-dimensional vectors.\n",
        "We can now add these directly to the token embeddings, where PyTorch will add the 4x256-\n",
        "dimensional pos_embeddings tensor to each 4x256-dimensional token embedding tensor in\n",
        "each of the 8 batches:\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "4bzdwocdHl-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ano_sohuDsHU",
        "outputId": "bf4c91c7-ada0-401c-a00f-6a2ae4e689c9"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LMeP_7UQDsFm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "knnwM-3aDsDg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9icIai4GDsAh"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}